{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeGameEnv(gym.Env):\n",
    "    \"\"\"\n",
    "        Maze Game Environment\n",
    "\n",
    "        The maze is represented as a 2D numpy array where:\n",
    "            'S' is the start position\n",
    "            'G' is the goal position\n",
    "            'X' is a wall\n",
    "            ' ' is an empty space\n",
    "\n",
    "        The agent can move in 4 directions:\n",
    "            0: Up\n",
    "            1: Down\n",
    "            2: Left\n",
    "            3: Right\n",
    "\n",
    "        The agent receives a reward of 1.0 when it reaches the goal position and 0.0 otherwise.\n",
    "\n",
    "        :param maze: 2D numpy array representing the maze\n",
    "        :type maze: np.ndarray\n",
    "\n",
    "        :param cell_size: Size of each cell in the maze\n",
    "        :type cell_size: int\n",
    "\n",
    "        :param screen: Pygame screen object\n",
    "        :type screen: pygame.Surface\n",
    "\n",
    "        :param start_pos: Start position of the agent\n",
    "        :type start_pos: Tuple[int, int]\n",
    "\n",
    "        :param goal_pos: Goal position of the agent\n",
    "        :type goal_pos: Tuple[int, int]\n",
    "\n",
    "        :param current_pos: Current position of the agent\n",
    "        :type current_pos: Tuple[int, int]\n",
    "\n",
    "        :param num_rows: Number of rows in the maze\n",
    "        :type num_rows: int\n",
    "\n",
    "        :param num_cols: Number of columns in the maze\n",
    "        :type num_cols: int\n",
    "\n",
    "        :param action_space: Action space of the environment\n",
    "        :type action_space: gym.spaces.Discrete\n",
    "\n",
    "        :param observation_space: Observation space of the environment\n",
    "        :type observation_space: gym.spaces.Tuple\n",
    "\n",
    "        :param screen: Pygame screen object\n",
    "        :type screen: pygame.Surface\n",
    "\n",
    "        :param cell_size: Size of each cell in the maze\n",
    "        :type cell_size: int\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, maze):\n",
    "        super().__init__()\n",
    "        self.maze = np.array(maze) # 2d array\n",
    "        self.start_pos = np.where(self.maze == 'S')\n",
    "        self.goal_pos = np.where(self.maze == 'G')\n",
    "        self.current_pos = self.start_pos\n",
    "        self.num_rows, self.num_cols = self.maze.shape\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Tuple((spaces.Discrete(self.num_rows), spaces.Discrete(self.num_cols)))\n",
    "\n",
    "        pygame.init()\n",
    "        self.cell_size = 125\n",
    "        self.screen = pygame.display.set_mode((self.num_cols * self.cell_size, self.num_rows * self.cell_size))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "            Reset agent position to start position\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self.current_pos = self.start_pos\n",
    "        return self.current_pos, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "            Updates agentâ€™s position according to the action taken and provide reward\n",
    "        \"\"\"\n",
    "        new_pos = np.array(self.current_pos)\n",
    "        if action == 0: # Up\n",
    "            new_pos[0] -= 1\n",
    "        if action == 1: # Dwon\n",
    "            new_pos[0] += 1\n",
    "        if action == 2: # Left\n",
    "            new_pos[1] -= 1\n",
    "        if action == 3:\n",
    "            new_pos[1] += 1\n",
    "\n",
    "        if self._is_valid_position(new_pos):\n",
    "            self.current_pos = new_pos\n",
    "\n",
    "        if np.array_equal(self.current_pos, self.goal_pos):\n",
    "            reward = 1.0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0.0\n",
    "            done = False\n",
    "\n",
    "        return self.current_pos, reward, done, False, {}\n",
    "\n",
    "    def _is_valid_position(self, pos):\n",
    "        row, col = pos\n",
    "\n",
    "        if row < 0 or col < 0 or row >= self.num_rows or col >= self.num_cols:\n",
    "            return False\n",
    "        \n",
    "        if self.maze[row, col] == '#':\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "            Render game environment using pygame by drawing elements for each cell by using nested loops. \n",
    "            You can simply print the maze grid as well, no necessary requirement for pygame\n",
    "        \"\"\"\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                cell_left = col * self.cell_size\n",
    "                cell_top = row * self.cell_size\n",
    "\n",
    "                # try:\n",
    "                #     print(np.array(self.current_pos)==np.array([row,col]).reshape(-1,1))\n",
    "                # except Exception as e:\n",
    "                #     print('Initial state')\n",
    "\n",
    "                if self.maze[row, col] == '#':\n",
    "                    pygame.draw.rect(self.screen, (0, 0, 0), (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "                elif self.maze[row, col] == 'S':\n",
    "                    pygame.draw.rect(self.screen, (0, 255, 0), (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "                elif self.maze[row, col] == 'G':\n",
    "                    pygame.draw.rect(self.screen, (255, 0, 0), (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "        \n",
    "                if np.array_equal(np.array(self.current_pos), np.array([row, col]).reshape(-1,1)):  # Agent position\n",
    "                    pygame.draw.rect(self.screen, (0, 0, 255), (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "\n",
    "        pygame.display.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(\n",
    "    id='MazeGame-v0',\n",
    "    entry_point=MazeGameEnv,\n",
    "    kwargs={'maze': None}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szymon/miniconda3/envs/IOwADC/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:133: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method should be an int or np.int64, actual type: <class 'numpy.ndarray'>\u001b[0m\n",
      "  logger.warn(f\"{pre} should be an int or np.int64, actual type: {type(obs)}\")\n",
      "/Users/szymon/miniconda3/envs/IOwADC/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/Users/szymon/miniconda3/envs/IOwADC/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/szymon/miniconda3/envs/IOwADC/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:133: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method should be an int or np.int64, actual type: <class 'numpy.ndarray'>\u001b[0m\n",
      "  logger.warn(f\"{pre} should be an int or np.int64, actual type: {type(obs)}\")\n",
      "/Users/szymon/miniconda3/envs/IOwADC/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Reward: 1.0\n",
      "Done: True\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "maze = [\n",
    "    ['S', '', '.', '.'],\n",
    "    ['.', '#', '.', '#'],\n",
    "    ['.', '.', '.', '.'],\n",
    "    ['#', '.', '#', 'G'],\n",
    "]\n",
    "\n",
    "env = gym.make('MazeGame-v0', maze=maze)\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "done = False\n",
    "while True:\n",
    "    pygame.event.get()\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    env.render()\n",
    "    print('Reward:', reward)\n",
    "    print('Done:', done)\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "    pygame.time.wait(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(episodes, is_training=True, render=False):\n",
    "    \"\"\"\n",
    "    Run the maze problem\n",
    "\n",
    "    :param episodes: number of episodes to run\n",
    "    :param is_training: if True, the agent will learn, otherwise it will use a pre-trained model\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    maze = [\n",
    "        ['S', '', '.', '.'],\n",
    "        ['.', '#', '.', '#'],\n",
    "        ['.', '.', '.', '.'],\n",
    "        ['#', '.', '#', 'G'],\n",
    "    ]\n",
    "\n",
    "    env = gym.make('MazeGame-v0', maze=maze)\n",
    "\n",
    "    if(is_training):\n",
    "        q = np.zeros((16, 4))\n",
    "    else:\n",
    "        f = open('maze_game.pkl', 'rb')\n",
    "        q = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "    learning_rate_a = 0.9\n",
    "    discount_factor_g = 0.9\n",
    "\n",
    "    epsilon = 1\n",
    "    epsilon_decay_rate = 0.0001\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        print(f'state={state}')\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        rewards=0\n",
    "\n",
    "        while (not terminated and not truncated):\n",
    "\n",
    "            if is_training and rng.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q[state, :])\n",
    "\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            if is_training:\n",
    "                q[state, action] = q[state, action] + learning_rate_a * (\n",
    "                    reward + discount_factor_g * np.max(q[new_state, :]) - q[state, action]\n",
    "                )\n",
    "\n",
    "            state = new_state\n",
    "            rewards += reward\n",
    "\n",
    "        epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "\n",
    "        if epsilon == 0:\n",
    "            learning_rate_a = 0.0001\n",
    "\n",
    "        if reward == 1:\n",
    "            rewards_per_episode[i] = 1\n",
    "\n",
    "        rewards_per_episode[i] = rewards\n",
    "\n",
    "        print(f'Episode {i+1}/{episodes}, rewards: {rewards}')\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    if is_training:\n",
    "        f = open('maze_game.pkl','wb')\n",
    "        pickle.dump(q, f)\n",
    "        f.close()\n",
    "\n",
    "    mean_rewards = np.zeros(episodes)\n",
    "    for t in range(episodes):\n",
    "        mean_rewards[t] = np.mean(rewards_per_episode[max(0, t-100):(t+1)])\n",
    "    plt.plot(mean_rewards)\n",
    "    plt.savefig(f'maze_game_mean.png')\n",
    "\n",
    "    sum_rewards = np.zeros(episodes)\n",
    "    for t in range(episodes):\n",
    "        sum_rewards[t] = np.sum(rewards_per_episode[max(0, t-100):(t+1)])\n",
    "    plt.plot(sum_rewards)\n",
    "    plt.savefig(f'maze_game_sum.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state=(array([0]), array([0]))\n",
      "Episode 1/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 2/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 3/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 4/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 5/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 6/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 7/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 8/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 9/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 10/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 11/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 12/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 13/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 14/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 15/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 16/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 17/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 18/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 19/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n",
      "Episode 20/15000, rewards: 1.0\n",
      "state=(array([0]), array([0]))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 1 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 52\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(episodes, is_training, render)\u001b[0m\n\u001b[1;32m     49\u001b[0m new_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[0;32m---> 52\u001b[0m     q[state, action] \u001b[38;5;241m=\u001b[39m \u001b[43mq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m learning_rate_a \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m     53\u001b[0m         reward \u001b[38;5;241m+\u001b[39m discount_factor_g \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(q[new_state, :]) \u001b[38;5;241m-\u001b[39m q[state, action]\n\u001b[1;32m     54\u001b[0m     )\n\u001b[1;32m     56\u001b[0m state \u001b[38;5;241m=\u001b[39m new_state\n\u001b[1;32m     57\u001b[0m rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for axis 1 with size 4"
     ]
    }
   ],
   "source": [
    "run(15000, is_training=True, render=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IOwADC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
